# ðŸ“‚ Load dataset directly from GitHub
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time, os, joblib
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, ConfusionMatrixDisplay, RocCurveDisplay

# -------------------------
# Load dataset
# -------------------------
url = "https://raw.githubusercontent.com/avinash972/restaurant-dw-ml/main/restaurant_orders.csv"
df_orders = pd.read_csv(url, parse_dates=['timestamp'])
print("Rows loaded:", len(df_orders))
df_orders.head()

# -------------------------
# Feature Engineering + Label Creation
# -------------------------
orders = df_orders.copy()
orders = orders.sort_values(['customer_id','timestamp']).reset_index(drop=True)

# Label: return within 30 days
comp = orders[orders['status']=='completed'].sort_values(['customer_id','timestamp']).reset_index(drop=True)
comp['next_ts'] = comp.groupby('customer_id')['timestamp'].shift(-1)
comp['days_to_next'] = (comp['next_ts'] - comp['timestamp']).dt.days
comp['return_30d'] = ((comp['days_to_next'].notna()) & (comp['days_to_next'] <= 30)).astype(int)
label_map = comp.set_index('order_id')['return_30d'].to_dict()
orders['return_30d'] = orders['order_id'].map(label_map).fillna(0).astype(int)

# Features
orders['prev_ts'] = orders.groupby('customer_id')['timestamp'].shift(1)
orders['recency_days'] = (orders['timestamp'] - orders['prev_ts']).dt.days.fillna(999).astype(int)
orders['avg_ticket'] = orders['total_amount'] / orders['num_items'].replace(0,1)
orders['order_hour'] = orders['timestamp'].dt.hour
orders['is_weekend'] = orders['timestamp'].dt.dayofweek.isin([5,6]).astype(int)

# 90-day frequency (fast version)
orders = orders.sort_values(['customer_id','timestamp'])
orders['freq_90d'] = orders.groupby('customer_id')['timestamp'].transform(
    lambda x: x.rolling('90D').count() - 1  # exclude current order
)
orders['freq_90d'] = orders['freq_90d'].fillna(0).astype(int)

# Final modeling dataset
model_df = orders[orders['status']=='completed'].copy()
print('Model dataset rows:', len(model_df))

# -------------------------
# Train & Evaluate Models
# -------------------------
split_date = pd.to_datetime('2024-07-01')
train = model_df[model_df['timestamp'] < split_date].copy()
test = model_df[model_df['timestamp'] >= split_date].copy()

feature_cols = ['total_amount','num_items','recency_days','avg_ticket','order_hour','is_weekend','freq_90d']

# Handle channel dummies
train = pd.get_dummies(train, columns=['channel'], drop_first=True)
test = pd.get_dummies(test, columns=['channel'], drop_first=True)
for c in ['channel_takeaway','channel_delivery']:
    if c not in train.columns: train[c]=0
    if c not in test.columns: test[c]=0
feature_cols += ['channel_takeaway','channel_delivery']

X_train, y_train = train[feature_cols].fillna(0), train['return_30d']
X_test, y_test = test[feature_cols].fillna(0), test['return_30d']

dt = DecisionTreeClassifier(max_depth=6, min_samples_leaf=5, random_state=42)
nb = GaussianNB()

dt.fit(X_train, y_train)
nb.fit(X_train, y_train)

y_dt, y_nb = dt.predict(X_test), nb.predict(X_test)
p_dt, p_nb = dt.predict_proba(X_test)[:,1], nb.predict_proba(X_test)[:,1]

# -------------------------
# Metrics
# -------------------------
def metrics(y_true,y_pred,y_proba):
    return dict(
        accuracy=round(accuracy_score(y_true,y_pred),3),
        precision=round(precision_score(y_true,y_pred,zero_division=0),3),
        recall=round(recall_score(y_true,y_pred,zero_division=0),3),
        f1=round(f1_score(y_true,y_pred,zero_division=0),3),
        roc_auc=round(roc_auc_score(y_true,y_proba) if len(set(y_true))>1 else float('nan'),3)
    )

dt_metrics = metrics(y_test,y_dt,p_dt)
nb_metrics = metrics(y_test,y_nb,p_nb)

print('Decision Tree:', dt_metrics)
print('Naive Bayes:', nb_metrics)
print('\nClassification Report (Decision Tree):\n', classification_report(y_test,y_dt,zero_division=0))
print('\nClassification Report (Naive Bayes):\n', classification_report(y_test,y_nb,zero_division=0))

# -------------------------
# Comparison Graphs
# -------------------------
# Bar plot of metrics
comp_df = pd.DataFrame([dt_metrics, nb_metrics], index=['Decision Tree','Naive Bayes'])
comp_df.plot(kind='bar', figsize=(10,6))
plt.title('Model Comparison: Decision Tree vs Naive Bayes')
plt.ylabel('Score')
plt.ylim(0,1)
plt.xticks(rotation=0)
plt.legend(loc='lower right')
plt.show()

# Confusion matrices
ConfusionMatrixDisplay.from_estimator(dt, X_test, y_test, cmap='Blues')
plt.title('Decision Tree Confusion Matrix')
plt.show()

ConfusionMatrixDisplay.from_estimator(nb, X_test, y_test, cmap='Oranges')
plt.title('Naive Bayes Confusion Matrix')
plt.show()

# ROC curve comparison
RocCurveDisplay.from_estimator(dt, X_test, y_test)
RocCurveDisplay.from_estimator(nb, X_test, y_test)
plt.title('ROC Curve Comparison')
plt.show()

# -------------------------
# Decision Tree Feature Importance
# -------------------------
feat_imp = pd.DataFrame({
    'feature': feature_cols,
    'importance': dt.feature_importances_
}).sort_values(by='importance', ascending=False)

plt.figure(figsize=(8,5))
sns.barplot(data=feat_imp, x='importance', y='feature', palette='viridis')
plt.title('Decision Tree Feature Importance')
plt.show()

# -------------------------
# Naive Bayes Feature Insights
# -------------------------
nb_params = pd.DataFrame({
    'feature': feature_cols,
    'mean_0': nb.theta_[0],
    'mean_1': nb.theta_[1],
    'var_0': nb.sigma_[0],
    'var_1': nb.sigma_[1]
})
nb_params

# -------------------------
# Save artifacts
# -------------------------
os.makedirs('outputs', exist_ok=True)
joblib.dump(dt, 'outputs/decision_tree.joblib')
joblib.dump(nb, 'outputs/naive_bayes.joblib')
model_df.to_csv('outputs/model_dataset.csv', index=False)
print('Artifacts saved in outputs/')
